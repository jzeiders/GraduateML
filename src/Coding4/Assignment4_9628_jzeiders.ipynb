{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# John Zeiders (jzeiders) - Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, det\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Estep(data, prob, mean, Sigma):\n",
    "    \"\"\"\n",
    "    E-step: Calculate responsibilities\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: ndarray, shape (n, p)\n",
    "        The input data matrix\n",
    "    prob: ndarray, shape (G,)\n",
    "        Mixing proportions\n",
    "    mean: ndarray, shape (p, G)\n",
    "        Mean vectors for each component\n",
    "    Sigma: ndarray, shape (p, p)\n",
    "        Shared covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    responsibilities: ndarray, shape (n, G)\n",
    "        Matrix of posterior probabilities P(Z_i=k|x_i)\n",
    "    \"\"\"\n",
    "    n, p  = data.shape\n",
    "    G = prob.shape[0]\n",
    "    \n",
    "    # Initialize matrix to store densities\n",
    "    densities = np.zeros((n, G))\n",
    "\n",
    "    # Precompute constants related to the covariance matrix\n",
    "    try:\n",
    "        inv_Sigma = np.linalg.inv(Sigma)  # Inverse of covariance matrix\n",
    "        det_Sigma = np.linalg.det(Sigma)  # Determinant of covariance matrix\n",
    "        if det_Sigma <= 0:\n",
    "            raise np.linalg.LinAlgError(\"Covariance matrix is not positive definite.\")\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        raise ValueError(\"Invalid covariance matrix. \" + str(e))\n",
    "\n",
    "    # Compute the normalization constant for the multivariate normal distribution\n",
    "    norm_const = 1.0 / (np.power((2 * np.pi), p / 2) * np.sqrt(det_Sigma))\n",
    "\n",
    "    # Compute densities for each Gaussian component\n",
    "    for k in range(G):\n",
    "        # Extract the mean vector for the k-th component\n",
    "        mu_k = mean[:, k]  # Shape: (p,)\n",
    "\n",
    "        # Compute the difference between each data point and the mean vector\n",
    "        diff = data - mu_k  # Shape: (n, p)\n",
    "\n",
    "        # Compute the exponent term for the multivariate normal PDF\n",
    "        # This represents: -0.5 * (x - mu_k)^T * inv_Sigma * (x - mu_k)\n",
    "        # Efficiently computed using einsum\n",
    "        exponent = -0.5 * np.einsum('ij,jk,ik->i', diff, inv_Sigma, diff)  # Shape: (n,)\n",
    "\n",
    "        # Compute the density for the k-th component\n",
    "        densities[:, k] = norm_const * np.exp(exponent)  # Shape: (n,)\n",
    "    \n",
    "    # Multiply by mixing proportions\n",
    "    weighted_densities = densities * prob[np.newaxis, :]\n",
    "    \n",
    "    # Normalize to get responsibilities\n",
    "    responsibilities = weighted_densities / np.sum(weighted_densities, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "def Mstep(data, responsibilities):\n",
    "    \"\"\"\n",
    "    M-step: Update parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: ndarray, shape (n, p)\n",
    "        The input data matrix\n",
    "    responsibilities: ndarray, shape (n, G)\n",
    "        Matrix of posterior probabilities\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    prob: ndarray, shape (G,)\n",
    "        Updated mixing proportions\n",
    "    mean: ndarray, shape (p, G)\n",
    "        Updated mean vectors\n",
    "    Sigma: ndarray, shape (p, p)\n",
    "        Updated shared covariance matrix\n",
    "    \"\"\"\n",
    "    n, p = data.shape\n",
    "    G = responsibilities.shape[1]\n",
    "    \n",
    "    # Update mixing proportions\n",
    "    prob = np.mean(responsibilities, axis=0)\n",
    "    \n",
    "    # Update means\n",
    "    mean = np.zeros((p, G))\n",
    "    for k in range(G):\n",
    "        mean[:, k] = np.sum(responsibilities[:, k:k+1] * data, axis=0) / np.sum(responsibilities[:, k])\n",
    "    \n",
    "    # Update shared covariance matrix\n",
    "    Sigma = np.zeros((p, p))\n",
    "    for k in range(G):\n",
    "        diff = data - mean[:, k]  # Shape: (n, p)\n",
    "        weighted_diff = responsibilities[:, k].reshape(n, 1) * diff  # Shape: (n, p)\n",
    "        Sigma += weighted_diff.T @ diff  # Shape: (p, p)\n",
    "    Sigma /= n\n",
    "    \n",
    "    return prob, mean, Sigma\n",
    "\n",
    "def loglik(data, prob, mean, Sigma):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: ndarray, shape (n, p)\n",
    "        The input data matrix\n",
    "    prob: ndarray, shape (G,)\n",
    "        Mixing proportions\n",
    "    mean: ndarray, shape (p, G)\n",
    "        Mean vectors\n",
    "    Sigma: ndarray, shape (p, p)\n",
    "        Shared covariance matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ll: float\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    G = prob.shape[0]\n",
    "    \n",
    "    # Initialize array for component densities\n",
    "    densities = np.zeros((n, G))\n",
    "    d = data.shape[1]\n",
    "    inv_Sigma = np.linalg.inv(Sigma)\n",
    "    det_Sigma = np.linalg.det(Sigma)\n",
    "    norm_const = 1.0 / (np.power((2 * np.pi), d / 2) * np.sqrt(det_Sigma))\n",
    "    \n",
    "    # Compute densities for each component\n",
    "    for k in range(G):\n",
    "            # Compute the difference between data and the k-th mean\n",
    "        diff = data - mean[:, k]  # Shape: (n_samples, d)\n",
    "        \n",
    "        # Compute the exponent term: -0.5 * (diff @ inv_Sigma * diff).sum(axis=1)\n",
    "        # Efficient computation using einsum for element-wise multiplication and summation\n",
    "        exponent = -0.5 * (diff @ inv_Sigma * diff).sum(axis=1)\n",
    "        \n",
    "        # Compute the density for the k-th component\n",
    "        densities[:, k] = prob[k] * norm_const * np.exp(exponent)\n",
    "    \n",
    "    # Sum over components and take log\n",
    "    ll = np.sum(np.log(np.sum(densities, axis=1)))\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def myEM(data, G, initial_params, itmax):\n",
    "    \"\"\"\n",
    "    Main EM algorithm function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: ndarray, shape (n, p)\n",
    "        The input data matrix\n",
    "    G: int\n",
    "        Number of components\n",
    "    initial_params: dict\n",
    "        Dictionary containing initial parameters:\n",
    "        'prob': mixing proportions\n",
    "        'mean': mean vectors\n",
    "        'Sigma': covariance matrix\n",
    "    itmax: int\n",
    "        Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    prob: ndarray, shape (G,)\n",
    "        Final mixing proportions\n",
    "    mean: ndarray, shape (p, G)\n",
    "        Final mean vectors\n",
    "    Sigma: ndarray, shape (p, p)\n",
    "        Final shared covariance matrix\n",
    "    ll: float\n",
    "        Final log-likelihood value\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    prob = initial_params['prob']\n",
    "    mean = initial_params['mean']\n",
    "    Sigma = initial_params['Sigma']\n",
    "    \n",
    "    for _ in range(itmax):\n",
    "        # E-step\n",
    "        responsibilities = Estep(data, prob, mean, Sigma)\n",
    "        \n",
    "        # M-step\n",
    "        prob, mean, Sigma = Mstep(data, responsibilities)\n",
    "        \n",
    "    # Compute final log-likelihood\n",
    "    ll = loglik(data, prob, mean, Sigma)\n",
    "    \n",
    "    return prob, mean, Sigma, ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_G2(data):\n",
    "    \"\"\"Test EM algorithm with G=2 components\"\"\"\n",
    "    n, p = data.shape\n",
    "    \n",
    "    # Set initial parameters as specified\n",
    "    p1 = 10/n\n",
    "    prob = np.array([p1, 1-p1])\n",
    "    \n",
    "    mean = np.zeros((p, 2))\n",
    "    mean[:, 0] = np.mean(data[:10], axis=0)  # mean of first 10 samples\n",
    "    mean[:, 1] = np.mean(data[10:], axis=0)  # mean of remaining samples\n",
    "    \n",
    "    # Calculate initial Sigma\n",
    "    diff1 = data[:10] - mean[:, 0]\n",
    "    diff2 = data[10:] - mean[:, 1]\n",
    "    Sigma = (diff1.T @ diff1 + diff2.T @ diff2) / n\n",
    "    \n",
    "    initial_params = {\n",
    "        'prob': prob,\n",
    "        'mean': mean,\n",
    "        'Sigma': Sigma\n",
    "    }\n",
    "    \n",
    "    # Run EM algorithm\n",
    "    final_prob, final_mean, final_Sigma, final_ll = myEM(data, 2, initial_params, 20)\n",
    "    \n",
    "    return final_prob, final_mean, final_Sigma, final_ll\n",
    "\n",
    "def test_G3(data):\n",
    "    \"\"\"Test EM algorithm with G=3 components\"\"\"\n",
    "    n, p = data.shape\n",
    "    \n",
    "    # Set initial parameters\n",
    "    p1, p2 = 10/n, 20/n\n",
    "    prob = np.array([p1, p2, 1-p1-p2])\n",
    "    \n",
    "    mean = np.zeros((p, 3))\n",
    "    mean[:, 0] = np.mean(data[:10], axis=0)      # mean of first 10 samples\n",
    "    mean[:, 1] = np.mean(data[10:30], axis=0)    # mean of next 20 samples\n",
    "    mean[:, 2] = np.mean(data[30:], axis=0)      # mean of remaining samples\n",
    "    \n",
    "    # Calculate initial Sigma\n",
    "    diff1 = data[:10] - mean[:, 0]\n",
    "    diff2 = data[10:30] - mean[:, 1]\n",
    "    diff3 = data[30:] - mean[:, 2]\n",
    "    Sigma = (diff1.T @ diff1 + diff2.T @ diff2 + diff3.T @ diff3) / n\n",
    "    \n",
    "    initial_params = {\n",
    "        'prob': prob,\n",
    "        'mean': mean,\n",
    "        'Sigma': Sigma\n",
    "    }\n",
    "    \n",
    "    # Run EM algorithm\n",
    "    final_prob, final_mean, final_Sigma, final_ll = myEM(data, 3, initial_params, 20)\n",
    "    \n",
    "    return final_prob, final_mean, final_Sigma, final_ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (272, 2)\n",
      "First 5 Data Points:\n",
      " [[ 3.6   79.   ]\n",
      " [ 1.8   54.   ]\n",
      " [ 3.333 74.   ]\n",
      " [ 2.283 62.   ]\n",
      " [ 4.533 85.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Load faithful dataset\n",
    "file_path = 'https://liangfgithub.github.io/Data/faithful.dat'  # Update this path as necessary\n",
    "data = np.loadtxt(file_path, skiprows=1, dtype=float, usecols=(1,2))\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"First 5 Data Points:\\n\", data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with G=2:\n",
      "\n",
      "prob\n",
      "[0.04297883 0.95702117]\n",
      "\n",
      "mean\n",
      "[[ 3.49564188  3.48743016]\n",
      " [76.79789154 70.63205853]]\n",
      "\n",
      "Sigma\n",
      "[[  1.29793612  13.92433626]\n",
      " [ 13.92433626 182.58009247]]\n",
      "\n",
      "loglik\n",
      "-1289.5693549424104\n"
     ]
    }
   ],
   "source": [
    "# Test G=2\n",
    "print(\"\\nTesting with G=2:\")\n",
    "prob2, mean2, Sigma2, ll2 = test_G2(data)\n",
    "\n",
    "print(\"\\nprob\")\n",
    "print(prob2)\n",
    "print(\"\\nmean\")\n",
    "print(mean2)\n",
    "print(\"\\nSigma\")\n",
    "print(Sigma2)\n",
    "print(\"\\nloglik\")\n",
    "print(ll2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with G=3:\n",
      "\n",
      "prob\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "\n",
      "mean\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "\n",
      "Sigma\n",
      "[[  1.26015772  13.51153756]\n",
      " [ 13.51153756 177.96419105]]\n",
      "\n",
      "loglik\n",
      "-1289.350958862739\n"
     ]
    }
   ],
   "source": [
    "# Test G=3\n",
    "print(\"\\nTesting with G=3:\")\n",
    "prob3, mean3, Sigma3, ll3 = test_G3(data)\n",
    "\n",
    "print(\"\\nprob\")\n",
    "print(prob3)\n",
    "print(\"\\nmean\")\n",
    "print(mean3)\n",
    "print(\"\\nSigma\")\n",
    "print(Sigma3)\n",
    "print(\"\\nloglik\")\n",
    "print(ll3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(data, w, A, B):\n",
    "    \"\"\"\n",
    "    Compute forward probabilities alpha(t,j)\n",
    "\n",
    "    Args:\n",
    "        data: T-by-1 observation sequence (1D array)\n",
    "        w: Initial state distribution (mz,)\n",
    "        A: Transition matrix (mz-by-mz)\n",
    "        B: Emission matrix (mz-by-mx)\n",
    "\n",
    "    Returns:\n",
    "        alpha: Forward probabilities (T-by-mz)\n",
    "    \"\"\"\n",
    "    T = len(data)\n",
    "    mz = len(w)\n",
    "    alpha = np.zeros((T, mz))\n",
    "\n",
    "    # Initialize first time step\n",
    "    alpha[0, :] = w * B[:, data[0]]\n",
    "\n",
    "    # Forward recursion\n",
    "    for t in range(1, T):\n",
    "        alpha[t, :] = np.dot(alpha[t - 1, :], A) * B[:, data[t]].T\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def backward_pass(data, A, B):\n",
    "    \"\"\"\n",
    "    Compute backward probabilities beta(t,j)\n",
    "\n",
    "    Args:\n",
    "        data: T-by-1 observation sequence (1D array)\n",
    "        A: Transition matrix (mz-by-mz)\n",
    "        B: Emission matrix (mz-by-mx)\n",
    "\n",
    "    Returns:\n",
    "        beta: Backward probabilities (T-by-mz)\n",
    "    \"\"\"\n",
    "    T = len(data)\n",
    "    mz = A.shape[0]\n",
    "    beta = np.zeros((T, mz))\n",
    "\n",
    "    # Initialize last time step\n",
    "    beta[-1, :] = 1\n",
    "\n",
    "    # Backward recursion\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        beta[t, :] = np.dot(A, (B[:, data[t + 1]] * beta[t + 1, :]))\n",
    "    \n",
    "    return beta\n",
    "\n",
    "def BW_onestep(data, w, A, B):\n",
    "    \"\"\"\n",
    "    One step of the Baum-Welch algorithm (E-step + M-step)\n",
    "\n",
    "    Args:\n",
    "        data: T-by-1 observation sequence (1D array)\n",
    "        w: Initial state distribution (mz,)\n",
    "        A: Current transition matrix (mz-by-mz)\n",
    "        B: Current emission matrix (mz-by-mx)\n",
    "\n",
    "    Returns:\n",
    "        A_new: Updated transition matrix\n",
    "        B_new: Updated emission matrix\n",
    "    \"\"\"\n",
    "    T = len(data)\n",
    "    mz = A.shape[0]\n",
    "    mx = B.shape[1]\n",
    "\n",
    "    # E-step: Compute forward and backward probabilities\n",
    "    alpha = forward_pass(data, w, A, B)\n",
    "    beta = backward_pass(data, A, B)\n",
    "\n",
    "    # Compute xi(t,i,j) = P(z_t=i, z_{t+1}=j | x_{1:T})\n",
    "    xi = np.zeros((T - 1, mz, mz))\n",
    "    for t in range(T - 1):\n",
    "        denominator = np.dot(np.dot(alpha[t, :], A) * B[:, data[t + 1]], beta[t + 1, :])\n",
    "        numerator = alpha[t, :, np.newaxis] * A * B[:, data[t + 1]] * beta[t + 1, :]\n",
    "        xi[t, :, :] = numerator / denominator\n",
    "    \n",
    "    # Compute state probabilities (gamma)\n",
    "    gammas_j = np.zeros((T, mz))\n",
    "    gammas_j[:-1] = np.sum(xi, axis=2)\n",
    "    # Fix for last time step - should use alpha and beta\n",
    "    gammas_j[-1] = (alpha[-1] * beta[-1]) / np.sum(alpha[-1] * beta[-1])\n",
    "\n",
    "    # M-step: Update parameters\n",
    "    # Update A\n",
    "    A_new = np.zeros_like(A)\n",
    "    for i in range(mz):\n",
    "        for j in range(mz):\n",
    "            numerator = np.sum(xi[:, i, j])\n",
    "            denominator = np.sum(xi[:, i, :])\n",
    "            A_new[i, j] = numerator / denominator\n",
    "\n",
    "    # Update B (vectorized version)\n",
    "    B_new = np.zeros_like(B)\n",
    "    for l in range(mx):\n",
    "        mask = (data == l)\n",
    "        B_new[:, l] = np.sum(gammas_j[mask], axis=0) / (np.sum(gammas_j, axis=0) + 1e-300)\n",
    "\n",
    "    # Verify row stochasticity\n",
    "    assert np.allclose(np.sum(A_new, axis=1), 1, rtol=1e-5), \"A matrix not row stochastic\"\n",
    "    assert np.allclose(np.sum(B_new, axis=1), 1, rtol=1e-5), \"B matrix not row stochastic\"\n",
    "\n",
    "    return A_new, B_new\n",
    "\n",
    "def myBW(data, initial_params, itmax):\n",
    "    \"\"\"\n",
    "    Main Baum-Welch algorithm with specified initial parameters\n",
    "\n",
    "    Args:\n",
    "        data: T-by-1 observation sequence (1D array)\n",
    "        initial_params: Dictionary containing initial parameters:\n",
    "            'w': Initial state distribution (mz,)\n",
    "            'A': Initial transition matrix (mz-by-mz)\n",
    "            'B': Initial emission matrix (mz-by-mx)\n",
    "        itmax: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        A_final: Final transition matrix\n",
    "        B_final: Final emission matrix\n",
    "        ll_final: Final log likelihood\n",
    "    \"\"\"\n",
    "    # Ensure data is a 1D array\n",
    "    data = np.asarray(data).flatten()\n",
    "\n",
    "    # Extract initial parameters\n",
    "    w = initial_params[\"w\"]\n",
    "    A = initial_params[\"A\"]\n",
    "    B = initial_params[\"B\"]\n",
    "\n",
    "    for iteration in range(itmax):\n",
    "        # Compute log likelihood\n",
    "        alpha = forward_pass(data, w, A, B)\n",
    "        current_ll = np.log(np.sum(alpha[-1]))\n",
    "        \n",
    "        # Update parameters\n",
    "        A, B = BW_onestep(data, w, A, B)\n",
    "\n",
    "\n",
    "    return A, B, current_ll\n",
    "\n",
    "def myViterbi(data, w, A, B):\n",
    "    \"\"\"\n",
    "    Compute the most likely state sequence using the Viterbi algorithm\n",
    "    \n",
    "    Args:\n",
    "        data: T-by-1 observation sequence (1D array)\n",
    "        w: Initial state distribution (mz,)\n",
    "        A: Transition matrix (mz-by-mz)\n",
    "        B: Emission matrix (mz-by-mx)\n",
    "    \n",
    "    Returns:\n",
    "        path: Most likely state sequence\n",
    "        max_prob: Probability of the most likely path\n",
    "    \"\"\"\n",
    "    # Ensure data is 1D array\n",
    "    data = np.asarray(data).flatten()\n",
    "    \n",
    "    T = len(data)        # Length of sequence\n",
    "    mz = len(w)          # Number of hidden states\n",
    "    \n",
    "    # Initialize tables\n",
    "    V = np.zeros((mz, T))  # Viterbi table\n",
    "    bp = np.zeros((mz, T), dtype=int)  # Backpointer table\n",
    "    \n",
    "    # Initialize first column of Viterbi table\n",
    "    V[:, 0] = np.log(w) + np.log(B[:, data[0]])\n",
    "    \n",
    "    # Forward pass: Fill tables\n",
    "    for t in range(1, T):\n",
    "        for j in range(mz):\n",
    "            # Calculate probabilities for all possible previous states\n",
    "            probs = V[:, t-1] + np.log(A[:, j]) + np.log(B[j, data[t]])\n",
    "            # Find maximum probability and its index\n",
    "            V[j, t] = np.max(probs)\n",
    "            bp[j, t] = np.argmax(probs)\n",
    "    \n",
    "    # Backward pass: Retrieve the most likely path\n",
    "    path = np.zeros(T, dtype=int)\n",
    "    # Find the most likely final state\n",
    "    path[-1] = np.argmax(V[:, -1])\n",
    "    max_prob = V[path[-1], -1]\n",
    "    \n",
    "    # Backtrack through the sequence\n",
    "    for t in range(T-2, -1, -1):\n",
    "        path[t] = bp[path[t+1], t+1]\n",
    "    \n",
    "    return path, max_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM Data Shape: (200,)\n",
      "First 5 Observations:\n",
      " [1 2 2 2 2]\n",
      "True State Sequence Shape: (200,)\n",
      "First 5 True States:\n",
      " [0. 0. 0. 0. 0.]\n",
      "\n",
      "Final Transition Matrix A:\n",
      "[[0.49793938 0.50206062]\n",
      " [0.44883431 0.55116569]]\n",
      "\n",
      "Final Emission Matrix B:\n",
      "[[0.22159897 0.20266127 0.57573976]\n",
      " [0.34175148 0.17866665 0.47958186]]\n",
      "\n",
      "Final Log-Likelihood:\n",
      "-202.3062728417872\n"
     ]
    }
   ],
   "source": [
    "# Load HMM data\n",
    "data_url = 'https://liangfgithub.github.io/Data/coding4_part2_data.txt'  # Update if necessary\n",
    "data = pd.read_table(data_url, sep=\"\\s+\", header=None).values.flatten() - 1\n",
    "print(\"HMM Data Shape:\", data.shape)\n",
    "print(\"First 5 Observations:\\n\", data[:5])\n",
    "\n",
    "# Load true state sequence (for comparison)\n",
    "Z_url = 'https://liangfgithub.github.io/Data/Coding4_part2_Z.txt'  # Update if necessary\n",
    "Z_true = pd.read_table(Z_url, sep=\"\\s+\", header=None).values.flatten() - 1  # Assuming states are 1-indexed\n",
    "Z_true = Z_true[~np.isnan(Z_true)]\n",
    "print(\"True State Sequence Shape:\", Z_true.shape)\n",
    "print(\"First 5 True States:\\n\", Z_true[:5])\n",
    "\n",
    "# Initialize parameters\n",
    "mz = 2  # number of hidden states\n",
    "mx = 3  # number of observation symbols\n",
    "\n",
    "w_initial = np.array([0.5, 0.5])\n",
    "A_initial = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "B_initial = np.array([[1/9, 3/9, 5/9], [1/6, 2/6, 3/6]])\n",
    "\n",
    "initial_params = {\n",
    "    'w': w_initial,\n",
    "    'A': A_initial,\n",
    "    'B': B_initial\n",
    "}\n",
    "\n",
    "# Run Baum-Welch algorithm\n",
    "itmax = 100\n",
    "A_final, B_final, ll_final = myBW(data, initial_params, itmax)\n",
    "\n",
    "print(\"\\nFinal Transition Matrix A:\")\n",
    "print(A_final)\n",
    "print(\"\\nFinal Emission Matrix B:\")\n",
    "print(B_final)\n",
    "print(\"\\nFinal Log-Likelihood:\")\n",
    "print(ll_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Likely State Sequence:\n",
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1]\n",
      "\n",
      "Probability of the Most Likely Path:\n",
      "-318.56245145850596\n",
      "\n",
      "Viterbi Path Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Run Viterbi algorithm\n",
    "path, max_prob = myViterbi(data, w_initial, A_final, B_final)\n",
    "\n",
    "print(\"\\nMost Likely State Sequence:\")\n",
    "print(path + 1)\n",
    "\n",
    "print(\"\\nProbability of the Most Likely Path:\")\n",
    "print(max_prob)\n",
    "\n",
    "# Compare with true states\n",
    "matches = (path == Z_true).sum()\n",
    "total = len(Z_true)\n",
    "accuracy = matches / total\n",
    "print(f\"\\nViterbi Path Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Baum-Welch with Uniform B for 20 Iterations:\n",
      "\n",
      "Transition Matrix A after 20 Iterations:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "Emission Matrix B after 20 Iterations:\n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n",
      "Log-Likelihood after 20 Iterations:\n",
      "-202.31544020689537\n",
      "\n",
      "Running Baum-Welch with Uniform B for 100 Iterations:\n",
      "\n",
      "Transition Matrix A after 100 Iterations:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "Emission Matrix B after 100 Iterations:\n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n",
      "Log-Likelihood after 100 Iterations:\n",
      "-202.31544020689537\n",
      "\n",
      "Running Baum-Welch with Uniform B for 20 Iterations:\n",
      "\n",
      "Transition Matrix A after 20 Iterations:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "Emission Matrix B after 20 Iterations:\n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n",
      "Log-Likelihood after 20 Iterations:\n",
      "-202.31544020689537\n",
      "\n",
      "Running Baum-Welch with Uniform B for 100 Iterations:\n",
      "\n",
      "Transition Matrix A after 100 Iterations:\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\n",
      "Emission Matrix B after 100 Iterations:\n",
      "[[0.285 0.19  0.525]\n",
      " [0.285 0.19  0.525]]\n",
      "\n",
      "Log-Likelihood after 100 Iterations:\n",
      "-202.31544020689537\n"
     ]
    }
   ],
   "source": [
    "# Initialize B uniformly\n",
    "B_uniform = np.full((mz, mx), 1/3)\n",
    "initial_params_uniform = {\n",
    "    'w': w_initial,\n",
    "    'A': A_initial,\n",
    "    'B': B_uniform\n",
    "}\n",
    "\n",
    "# Run Baum-Welch for 20 iterations\n",
    "print(\"\\nRunning Baum-Welch with Uniform B for 20 Iterations:\")\n",
    "A_final_20, B_final_20, ll_final_20 = myBW(data, initial_params_uniform, itmax=20)\n",
    "\n",
    "print(\"\\nTransition Matrix A after 20 Iterations:\")\n",
    "print(A_final_20)\n",
    "print(\"\\nEmission Matrix B after 20 Iterations:\")\n",
    "print(B_final_20)\n",
    "print(\"\\nLog-Likelihood after 20 Iterations:\")\n",
    "print(ll_final_20)\n",
    "\n",
    "# Run Baum-Welch for 100 iterations\n",
    "print(\"\\nRunning Baum-Welch with Uniform B for 100 Iterations:\")\n",
    "A_final_100, B_final_100, ll_final_100 = myBW(data, initial_params_uniform, itmax=100)\n",
    "\n",
    "print(\"\\nTransition Matrix A after 100 Iterations:\")\n",
    "print(A_final_100)\n",
    "print(\"\\nEmission Matrix B after 100 Iterations:\")\n",
    "print(B_final_100)\n",
    "print(\"\\nLog-Likelihood after 100 Iterations:\")\n",
    "print(ll_final_100)\n",
    "\n",
    "\n",
    "B_uniform[0, :] = np.array([[1/4, 1/2, 1/4]])\n",
    "B_uniform[1, :] = np.array([[1/4, 1/2, 1/4]])\n",
    "initial_params_uniform = {\n",
    "    'w': w_initial,\n",
    "    'A': A_initial,\n",
    "    'B': B_uniform\n",
    "}\n",
    "\n",
    "# Run Baum-Welch for 20 iterations\n",
    "print(\"\\nRunning Baum-Welch with Uniform B for 20 Iterations:\")\n",
    "A_final_20, B_final_20, ll_final_20 = myBW(data, initial_params_uniform, itmax=20)\n",
    "\n",
    "print(\"\\nTransition Matrix A after 20 Iterations:\")\n",
    "print(A_final_20)\n",
    "print(\"\\nEmission Matrix B after 20 Iterations:\")\n",
    "print(B_final_20)\n",
    "print(\"\\nLog-Likelihood after 20 Iterations:\")\n",
    "print(ll_final_20)\n",
    "\n",
    "# Run Baum-Welch for 100 iterations\n",
    "print(\"\\nRunning Baum-Welch with Uniform B for 100 Iterations:\")\n",
    "A_final_100, B_final_100, ll_final_100 = myBW(data, initial_params_uniform, itmax=100)\n",
    "\n",
    "print(\"\\nTransition Matrix A after 100 Iterations:\")\n",
    "print(A_final_100)\n",
    "print(\"\\nEmission Matrix B after 100 Iterations:\")\n",
    "print(B_final_100)\n",
    "print(\"\\nLog-Likelihood after 100 Iterations:\")\n",
    "print(ll_final_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm stabilizes after exactly one iteration. This makes sense as algorithm is unable to preference any of the hidden states as their outcomes are identical. This doesn't even require a constant value across the matrix, just as long as the rows of B are identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
